{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cd7bee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T12:28:48.485996Z",
     "start_time": "2022-05-23T12:28:48.435985Z"
    }
   },
   "outputs": [],
   "source": [
    "from mmsdk import mmdatasdk as md\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "DATA_DIR = \"cmumosei_highlevel\"\n",
    "LABEL_DIR = \"cmumosei_labels\"\n",
    "CUDA = torch.cuda.is_available()\n",
    "POSE_FEATURES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcb650c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T12:32:14.474811Z",
     "start_time": "2022-05-23T12:28:48.938097Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define your different modalities - refer to the filenames of the CSD files\n",
    "visual_field = 'CMU_MOSEI_VisualFacet42'\n",
    "acoustic_field = 'CMU_MOSEI_COVAREP'\n",
    "hand_r_features = \"hand_r_features_top10\"\n",
    "hand_l_features = \"hand_l_features_top10\"\n",
    "pose_features = \"pose_features_top10\"\n",
    "text_field = 'CMU_MOSEI_TimestampedWordVectors'\n",
    "\n",
    "features = [\n",
    "    text_field,\n",
    "    visual_field,\n",
    "    acoustic_field]\n",
    "\n",
    "open_features = [pose_features,\n",
    "    hand_l_features,\n",
    "    hand_r_features,\n",
    "    text_field,\n",
    "    visual_field,\n",
    "    acoustic_field]\n",
    "if POSE_FEATURES:\n",
    "    recipe = {feat: os.path.join(DATA_DIR, feat) +\n",
    "          '.csd' for feat in open_features}\n",
    "else:\n",
    "    recipe = {feat: os.path.join(DATA_DIR, feat) +\n",
    "            '.csd' for feat in features}\n",
    "dataset = md.mmdataset(recipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfc4cc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T12:32:14.490814Z",
     "start_time": "2022-05-23T12:32:14.479812Z"
    }
   },
   "outputs": [],
   "source": [
    "total_videos = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15a2ac8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T12:38:11.603944Z",
     "start_time": "2022-05-23T12:38:11.593943Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove videos not in Pose\n",
    "if POSE_FEATURES:\n",
    "    remove = [name for name in list(dataset[visual_field].keys()) + list(dataset[acoustic_field].keys()) + list(\n",
    "        dataset[text_field].keys()) + list(dataset[pose_features].keys()) if name not in set(dataset[pose_features].keys())]\n",
    "    for v in remove:\n",
    "        dataset.remove_id(v)\n",
    "remove = [name for name in list(dataset[visual_field].keys()) + list(dataset[acoustic_field].keys()) + list(\n",
    "    dataset[text_field].keys()) if name not in list(dataset[text_field].keys())[:total_videos]]\n",
    "for v in remove:\n",
    "    dataset.remove_id(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c3cd58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T12:38:57.466145Z",
     "start_time": "2022-05-23T12:38:13.301322Z"
    }
   },
   "outputs": [],
   "source": [
    "# Collapse\n",
    "# we define a simple averaging function that does not depend on intervals\n",
    "def avg(intervals: np.array, features: np.array) -> np.array:\n",
    "    try:\n",
    "        return np.average(features, axis=0)\n",
    "    except:\n",
    "        return features\n",
    "        \n",
    "# first we align to words with averaging, collapse_function receives a list of functions\n",
    "#dataset.align(pose_features, collapse_functions=[avg])\n",
    "dataset.align(text_field, collapse_functions=[avg])\n",
    "#aligned = dataset.align(text_field, replace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cc8a35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T12:39:18.638855Z",
     "start_time": "2022-05-23T12:38:57.468145Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_field = 'CMU_MOSEI_Labels'\n",
    "# we add and align to lables to obtain labeled segments\n",
    "# this time we don't apply collapse functions so that the temporal sequences are preserved\n",
    "label_recipe = {label_field: os.path.join(LABEL_DIR, label_field + '.csd')}\n",
    "dataset.add_computational_sequences(label_recipe, destination=None)\n",
    "dataset.align(label_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "034458c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alignment compatible': True,\n",
       " 'computational sequence description': 'Labels for CMU-MOSEI Dataset',\n",
       " 'computational sequence version': 1.0,\n",
       " 'contact': 'abagherz@andrew.cmu.edu',\n",
       " 'creator': 'Amir Zadeh',\n",
       " 'dataset bib citation': '@inproceedings{cmumoseiacl2018, title={Multimodal Language Analysis in the Wild: {CMU-MOSEI} Dataset and Interpretable Dynamic Fusion Graph}, author={Zadeh, Amir and Liang, Paul Pu and Vanbriesen, Jon and Poria, Soujanya and Cambria, Erik and Chen, Minghai and Morency, Louis-Philippe},booktitle={Association for Computational Linguistics (ACL)},year={2018}}',\n",
       " 'dataset name': 'CMU-MOSEI',\n",
       " 'dataset version': 1.0,\n",
       " 'dimension names': ['sentiment',\n",
       "  'happy',\n",
       "  'sad',\n",
       "  'anger',\n",
       "  'surprise',\n",
       "  'disgust',\n",
       "  'fear'],\n",
       " 'featureset bib citation': '@online{amt, author = {Amazon},title = {Amazon Mechanical Turk},year = {2017},url = {https://www.mturk.com}}',\n",
       " 'md5': None,\n",
       " 'root name': 'All Labels',\n",
       " 'uuid': 'bbce9ca9-e556-46f4-823e-7c5e0147afab'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[label_field].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8e146c",
   "metadata": {},
   "source": [
    "## Determining the Highest Sentiment Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948546f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T12:43:54.016992Z",
     "start_time": "2022-05-23T12:43:53.996988Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import heapq\n",
    "# Filter videos based per name (Merge segments)\n",
    "pattern = re.compile('(.*)\\[.*\\]')\n",
    "videos = set()\n",
    "for video in list(dataset[label_field].keys()):\n",
    "    vid_id = re.search(pattern, video).group(1)\n",
    "    videos.add(vid_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb259eef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T12:43:54.320059Z",
     "start_time": "2022-05-23T12:43:54.292053Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dictionary of VideoKey -> [Sentiments]\n",
    "dict_average_sentiment = dict()\n",
    "for video in list(dataset[label_field].keys()):\n",
    "    video_name = video.split(\"[\")[0]\n",
    "    if video_name in dict_average_sentiment.keys():\n",
    "        dict_average_sentiment[video_name] = np.vstack(\n",
    "            (dict_average_sentiment[video_name], dataset[label_field][video]['features'][0]))\n",
    "    else:\n",
    "        dict_average_sentiment[video_name] = dataset[label_field][video]['features'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cdc4fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T12:43:54.624127Z",
     "start_time": "2022-05-23T12:43:54.613125Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate the average sentiment for each video\n",
    "dict_avg_sentiment_q = []\n",
    "for k, v in dict_average_sentiment.items():\n",
    "    if len(v.shape) < 2:\n",
    "        n_segements = 1\n",
    "        avg_sentiments = v\n",
    "    else:\n",
    "        n_segements = v.shape[0]\n",
    "        avg_sentiments = v.mean(axis=0)\n",
    "    heapq.heappush(dict_avg_sentiment_q, (avg_sentiments[0], k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65dbe8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T12:43:55.051729Z",
     "start_time": "2022-05-23T12:43:55.030725Z"
    }
   },
   "outputs": [],
   "source": [
    "# Top 25 Positive Sentiment Videos\n",
    "pos_videos = [k for v, k in heapq.nlargest(25, dict_avg_sentiment_q)]\n",
    "# Top 50 Negative Videos\n",
    "neg_videos = [k for v, k in heapq.nsmallest(50, dict_avg_sentiment_q)]\n",
    "import random\n",
    "random.shuffle(dict_avg_sentiment_q)\n",
    "# 15 Neutral videos (sentiment average is -0.05 < v < 0.05)\n",
    "neutral_videos = [k for v, k in dict_avg_sentiment_q if np.abs(v) < 0.05][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093fd1a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T12:43:55.385804Z",
     "start_time": "2022-05-23T12:43:55.382803Z"
    }
   },
   "outputs": [],
   "source": [
    "#np.savetxt(\"aligned_videos.csv\", np.array(\n",
    "#    list(videos)), fmt=\"%s\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e606857",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:35:25.461164Z",
     "start_time": "2022-05-23T13:35:25.448799Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_split, test_split, _, _ = train_test_split(\n",
    "    list(videos), list(videos), test_size=0.2)\n",
    "train_split, dev_split, _, _ = train_test_split(train_split, train_split, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead94eba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:35:25.664016Z",
     "start_time": "2022-05-23T13:35:25.652332Z"
    }
   },
   "outputs": [],
   "source": [
    "train_split, dev_split, test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec009ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:35:26.365933Z",
     "start_time": "2022-05-23T13:35:26.250733Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_dev_test_splits(dataset, train_videos, test_videos, dev_videos, label_n=0, n_classes=7, verbose=False):\n",
    "    # Label_n is which label to predict if label_n = 0 and n_cl\n",
    "    # a sentinel epsilon for safe division, without it we will replace illegal values with a constant\n",
    "    EPS = 0\n",
    "    # place holders for the final train/dev/test dataset\n",
    "    train = []\n",
    "    dev = []\n",
    "    test = []\n",
    "\n",
    "    # define a regular expression to extract the video ID out of the keys\n",
    "    pattern = re.compile('(.*)\\[.*\\]')\n",
    "    num_drop = 0  # a counter to count how many data points went into some processing issues\n",
    "\n",
    "    for segment in dataset[label_field].keys():\n",
    "        # get the video ID and the features out of the aligned dataset\n",
    "        vid = re.search(pattern, segment).group(1)\n",
    "        label = deepcopy(dataset[label_field][segment]['features'])\n",
    "        # Uncomment to get sentiment only\n",
    "        if label_n == 0 and n_classes == -1:\n",
    "            # This means it will return the average sentiment (For Regression)\n",
    "            label = label[:,0]\n",
    "        elif label_n == 0 and n_classes == 2:\n",
    "            # This will return two classes Positive (1), Negative (0)\n",
    "            label = label[:,0]\n",
    "            label[label >= 0] = 1\n",
    "            label[label < 0] = 0\n",
    "        elif label_n == 0 and n_classes == 7:\n",
    "            # Returns Sentiment in 7 classes, [0, 6], 0 - Very Negative, 6 - Very Positive\n",
    "            # First Rounds to closest Int and sums +3 to convert it to [0,6]\n",
    "            label = np.round(label[:,0])+3\n",
    "        elif label_n >= 1 and label_n <= 6:\n",
    "            label = np.round(label[:,label_n])\n",
    "        else:\n",
    "            print(f\"ERROR: This is not a valid configuration. Parameters: label_n: {label_n} & n_classes: {n_classes}\")\n",
    "            raise(ValueError)\n",
    "        # Uncomment if you want rounded sentiment (+3) -> -3 becomes 0, this is just to \n",
    "        # make it easier to run on PyTorch.\n",
    "        #label = np.round(dataset[label_field][segment]['features'][:,0])+3\n",
    "        _words = dataset[text_field][segment]['features']\n",
    "        _visual = dataset[visual_field][segment]['features']\n",
    "        _acoustic = dataset[acoustic_field][segment]['features']\n",
    "        # Need this because some segments does not exist in pose\n",
    "        if POSE_FEATURES:\n",
    "            if not segment in dataset[pose_features].keys():\n",
    "                if verbose:\n",
    "                    print(f\"Havent found features for {segment}. Continouing\")\n",
    "                continue\n",
    "            _pose = dataset[pose_features][segment]['features']\n",
    "            _hand_r = dataset[hand_r_features][segment]['features']\n",
    "            _hand_l = dataset[hand_l_features][segment]['features']\n",
    "\n",
    "        # if the sequences are not same length after alignment, there must be some problem with some modalities\n",
    "        # we should drop it or inspect the data again\n",
    "        if POSE_FEATURES:\n",
    "            if not _words.shape[0] == _visual.shape[0] == _acoustic.shape[0] == _pose.shape[0] == _hand_r.shape[0] == _hand_l.shape[0]:\n",
    "                dif = np.abs(_visual.shape[0] - _pose.shape[0]) \n",
    "                if dif <= 3:\n",
    "                    intervals = dataset[pose_features][segment]['intervals']\n",
    "                    word_intervals = dataset[text_field][segment]['intervals']\n",
    "                    dif_intervals = np.array([intervals[:,0], word_intervals[:-dif,0]]).std(axis=0)\n",
    "                    #_new_pose_intervals = np.zeros((_visual.shape[0], 2))\n",
    "                    _new_pose = np.zeros((_visual.shape[0], _pose.shape[1]))\n",
    "                    last_i = 0\n",
    "                    h = []\n",
    "                    for i in range(len(intervals)):\n",
    "                        heapq.heappush(h, (dif_intervals[i],i))\n",
    "                    index_to_change = heapq.nlargest(dif, h)\n",
    "                    for _, i_c  in index_to_change:\n",
    "                        if last_i > 0:\n",
    "                            #_new_pose_intervals[last_i+1:i_c+1,:] = intervals[last_i:i_c,:]\n",
    "                            _new_pose[last_i+1:i_c+1,:] = _pose[last_i:i_c,:]\n",
    "                        else:\n",
    "                            #_new_pose_intervals[last_i:i_c,:] = intervals[last_i:i_c,:]\n",
    "                            _new_pose[last_i:i_c,:] = _pose[last_i:i_c,:]\n",
    "                        #_new_pose_intervals[i_c] = intervals[i_c-2:i_c+1,:].mean(axis=0)\n",
    "                        _new_pose[i_c] = _pose[i_c-2:i_c+1,:].mean(axis=0)\n",
    "                        last_i = i_c\n",
    "                    _new_pose[last_i+1:,:] = _pose[i_c-dif+1:,:]\n",
    "                    #_new_pose_intervals[last_i+1:,:] = intervals[i_c-dif+1:,:]\n",
    "                    _pose = np.concatenate((_pose, _pose[:dif:,:].reshape(dif,-1)))\n",
    "                    _hand_l = np.concatenate((_hand_l, _hand_l[:dif:,:].reshape(dif,-1)))\n",
    "                    _hand_r = np.concatenate((_hand_r, _hand_r[:dif:,:].reshape(dif,-1)))\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        print(f\"DROPPED: {vid},{segment} with text shape {_words.shape}, visual shape {_visual.shape}, acoustic shape {_acoustic.shape}, pose shape {_pose.shape}\")\n",
    "                    num_drop += 1\n",
    "                    continue\n",
    "        else:\n",
    "            if not _words.shape[0] == _visual.shape[0] == _acoustic.shape[0]:\n",
    "                if verbose:\n",
    "                    print(f\"DROPPED: {vid},{segment} with text shape {_words.shape}, visual shape {_visual.shape}, acoustic shape {_acoustic.shape}\")\n",
    "                num_drop += 1\n",
    "                continue\n",
    "        if verbose:\n",
    "            print(f\"Added: {segment}\")\n",
    "        # remove nan values\n",
    "        label = np.nan_to_num(label)\n",
    "        _visual = np.nan_to_num(_visual)\n",
    "        _acoustic = np.nan_to_num(_acoustic)\n",
    "        if POSE_FEATURES:\n",
    "            _pose = np.nan_to_num(_pose)\n",
    "            _hand_r = np.nan_to_num(_hand_r)\n",
    "            _hand_l = np.nan_to_num(_hand_l)\n",
    "\n",
    "        # remove speech pause tokens - this is in general helpful\n",
    "        # we should remove speech pauses and corresponding visual/acoustic features together\n",
    "        # otherwise modalities would no longer be aligned\n",
    "        words = []\n",
    "        visual = []\n",
    "        acoustic = []\n",
    "        if POSE_FEATURES:\n",
    "            pose = []\n",
    "            hand_r = []\n",
    "            hand_l = []\n",
    "        for i, word in enumerate(_words):\n",
    "            if word[0] != b'sp':\n",
    "                # SDK stores strings as bytes, decode into strings here\n",
    "                words.append(_words[i,:])\n",
    "                visual.append(_visual[i, :])\n",
    "                acoustic.append(_acoustic[i, :])\n",
    "                if POSE_FEATURES:\n",
    "                    pose.append(_pose[i, :])\n",
    "                    hand_r.append(_hand_r[i, :])\n",
    "                    hand_l.append(_hand_l[i, :])\n",
    "\n",
    "        words = np.asarray(words)\n",
    "        visual = np.asarray(visual)\n",
    "        acoustic = np.asarray(acoustic)\n",
    "        if POSE_FEATURES:\n",
    "            pose = np.asarray(pose)\n",
    "            hand_r = np.asarray(hand_r)\n",
    "            hand_l = np.asarray(hand_l)\n",
    "\n",
    "        # z-normalization per instance and remove nan/infs\n",
    "        visual = np.nan_to_num((visual - visual.mean(0, keepdims=True)) /\n",
    "                               (EPS + np.std(visual, axis=0, keepdims=True)))\n",
    "        acoustic = np.nan_to_num((acoustic - acoustic.mean(0, keepdims=True)) /\n",
    "                                 (EPS + np.std(acoustic, axis=0, keepdims=True)))\n",
    "        if POSE_FEATURES:\n",
    "            pose = np.nan_to_num((pose - pose.mean(0, keepdims=True)) /\n",
    "                                (EPS + np.std(pose, axis=0, keepdims=True)))\n",
    "            hand_r = np.nan_to_num((hand_r - hand_r.mean(0, keepdims=True)) /\n",
    "                                (EPS + np.std(hand_r, axis=0, keepdims=True)))\n",
    "            hand_l = np.nan_to_num((hand_l - hand_l.mean(0, keepdims=True)) /\n",
    "                                (EPS + np.std(hand_l, axis=0, keepdims=True)))\n",
    "        if POSE_FEATURES:\n",
    "            if vid in train_videos:\n",
    "                train.append(((words, visual, acoustic, pose, hand_r, hand_l), label, segment))\n",
    "            elif vid in dev_videos:\n",
    "                dev.append(((words, visual, acoustic, pose, hand_r, hand_l), label, segment))\n",
    "            elif vid in test_videos:\n",
    "                test.append(((words, visual, acoustic, pose, hand_r, hand_l), label, segment))\n",
    "            else:\n",
    "                print(f\"Found video that doesn't belong to any splits: {vid}\")\n",
    "        else:\n",
    "            if vid in train_videos:\n",
    "                train.append(((words, visual, acoustic), label, segment))\n",
    "            elif vid in dev_videos:\n",
    "                dev.append(((words, visual, acoustic), label, segment))\n",
    "            elif vid in test_videos:\n",
    "                test.append(((words, visual, acoustic), label, segment))\n",
    "            else:\n",
    "                print(f\"Found video that doesn't belong to any splits: {vid}\")\n",
    "    return train, dev, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b86d8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:35:26.474835Z",
     "start_time": "2022-05-23T13:35:26.455224Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's see the size of each set and shape of data\n",
    "train, dev, test = get_train_dev_test_splits(dataset, train_split, dev_split, test_split, label_n=0, n_classes=-1)\n",
    "print(len(train))\n",
    "print(len(dev))\n",
    "print(len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69ce20a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:35:27.408116Z",
     "start_time": "2022-05-23T13:35:27.402001Z"
    }
   },
   "outputs": [],
   "source": [
    "np.array(train).shape, np.array(dev).shape, np.array(test).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882161b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0][0][0].shape, train[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5475d9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling\n",
    "def multi_collate_reg(batch):\n",
    "    '''\n",
    "    Collate functions assume batch = [Dataset[i] for i in index_set]\n",
    "    '''\n",
    "    # for later use we sort the batch in descending order of length\n",
    "    batch = sorted(batch, key=lambda x: x[0][0].shape[0], reverse=True)\n",
    "    # get the data out of the batch - use pad sequence util functions from PyTorch to pad things\n",
    "    labels = torch.cat([torch.FloatTensor(sample[1],)\n",
    "                       for sample in batch], dim=0)\n",
    "                       \n",
    "    sentences = pad_sequence([torch.FloatTensor(sample[0][0])\n",
    "                             for sample in batch], padding_value=1)\n",
    "    visual = pad_sequence([torch.FloatTensor(sample[0][1])\n",
    "                          for sample in batch])\n",
    "\n",
    "    acoustic = pad_sequence([torch.FloatTensor(sample[0][2])\n",
    "                            for sample in batch])\n",
    "    if POSE_FEATURES:\n",
    "        pose = pad_sequence([torch.FloatTensor(sample[0][3]) for sample in batch])\n",
    "\n",
    "        hand_r = pad_sequence([torch.FloatTensor(sample[0][4]) for sample in batch])\n",
    "\n",
    "        hand_l = pad_sequence([torch.FloatTensor(sample[0][5]) for sample in batch])\n",
    "\n",
    "    # lengths are useful later in using RNNs\n",
    "    lengths = torch.LongTensor([sample[0][0].shape[0] for sample in batch])\n",
    "    if POSE_FEATURES:\n",
    "        return sentences, visual, acoustic, pose, hand_r, hand_l, labels, lengths\n",
    "    else:\n",
    "        return sentences, visual, acoustic, labels, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c19af1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:35:27.781446Z",
     "start_time": "2022-05-23T13:35:27.756441Z"
    }
   },
   "outputs": [],
   "source": [
    "# Modelling\n",
    "def multi_collate_class(batch):\n",
    "    '''\n",
    "    Collate functions assume batch = [Dataset[i] for i in index_set]\n",
    "    '''\n",
    "    # for later use we sort the batch in descending order of length\n",
    "    batch = sorted(batch, key=lambda x: x[0][0].shape[0], reverse=True)\n",
    "    # get the data out of the batch - use pad sequence util functions from PyTorch to pad things\n",
    "    labels = torch.cat([torch.LongTensor(sample[1],)\n",
    "                       for sample in batch], dim=0)\n",
    "                       \n",
    "    sentences = pad_sequence([torch.FloatTensor(sample[0][0])\n",
    "                             for sample in batch], padding_value=1)\n",
    "    visual = pad_sequence([torch.FloatTensor(sample[0][1])\n",
    "                          for sample in batch])\n",
    "\n",
    "    acoustic = pad_sequence([torch.FloatTensor(sample[0][2])\n",
    "                            for sample in batch])\n",
    "    if POSE_FEATURES:\n",
    "        pose = pad_sequence([torch.FloatTensor(sample[0][3]) for sample in batch])\n",
    "\n",
    "        hand_r = pad_sequence([torch.FloatTensor(sample[0][4]) for sample in batch])\n",
    "\n",
    "        hand_l = pad_sequence([torch.FloatTensor(sample[0][5]) for sample in batch])\n",
    "\n",
    "    # lengths are useful later in using RNNs\n",
    "    lengths = torch.LongTensor([sample[0][0].shape[0] for sample in batch])\n",
    "    if POSE_FEATURES:\n",
    "        return sentences, visual, acoustic, pose, hand_r, hand_l, labels, lengths\n",
    "    else:\n",
    "        return sentences, visual, acoustic, labels, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, shuffle=False, batch_size=32, collate_fn=multi_collate_reg)\n",
    "dev_loader = DataLoader(dev, shuffle=False, batch_size=32, collate_fn=multi_collate_reg)\n",
    "test_loader = DataLoader(test, shuffle=False, batch_size=32, collate_fn=multi_collate_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f116630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangLSTM(nn.Module):\n",
    "    def __init__(self, input_sizes, hidden_sizes, output_size, dropout_rate, n_layers=2, classification=False):\n",
    "        super(LangLSTM, self).__init__()\n",
    "        self.input_size = input_sizes\n",
    "        self.hidden_size = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.classification = classification\n",
    "\n",
    "        #self.embed = nn.Embedding(len(word_dict), input_sizes[0])\n",
    "\n",
    "        self.total_inputsize = sum(self.input_size)\n",
    "        self.total_hiddensize = sum(self.hidden_size)\n",
    "        self.biLSTM = nn.LSTM(\n",
    "            self.total_inputsize, self.total_hiddensize, bidirectional=True, num_layers = n_layers, dropout=dropout_rate)\n",
    "\n",
    "        self.output = nn.Linear(self.total_hiddensize*2*n_layers, output_size)\n",
    "        if self.classification:\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "        # self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, sentences, visual, acoustic, pose, hand_r, hand_l, lengths):\n",
    "        batch_size = lengths.size(0)\n",
    "        #sentences = self.embed(sentences)\n",
    "        packed_sequence = pack_padded_sequence(sentences, lengths)\n",
    "        _, (final_h, _) = self.biLSTM(packed_sequence)\n",
    "        h = final_h.view(batch_size, -1)\n",
    "        o = self.output(h)\n",
    "        #o = torch.cat((o[:,0].unsqueeze(dim=1), self.relu(o[:,1:])),axis=1)\n",
    "        if self.classification:\n",
    "            return self.softmax(o)\n",
    "        return o.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979754de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:35:28.804470Z",
     "start_time": "2022-05-23T13:35:28.796468Z"
    }
   },
   "outputs": [],
   "source": [
    "class EFLSTMP(nn.Module):\n",
    "    def __init__(self, input_sizes, hidden_sizes, output_size, dropout_rate, n_layers=2, classification=False):\n",
    "        super(EFLSTMP, self).__init__()\n",
    "        self.input_size = input_sizes\n",
    "        self.hidden_size = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.classification = classification\n",
    "\n",
    "        #self.embed = nn.Embedding(len(word_dict), input_sizes[0])\n",
    "\n",
    "        self.total_inputsize = sum(self.input_size)\n",
    "        self.total_hiddensize = sum(self.hidden_size)\n",
    "        self.biLSTM = nn.LSTM(\n",
    "            self.total_inputsize, self.total_hiddensize, bidirectional=True, num_layers = n_layers, dropout=dropout_rate)\n",
    "\n",
    "        self.output = nn.Linear(self.total_hiddensize*2*n_layers, output_size)\n",
    "        if self.classification:\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "        # self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, sentences, visual, acoustic, pose, hand_r, hand_l, lengths):\n",
    "        batch_size = lengths.size(0)\n",
    "        #sentences = self.embed(sentences)\n",
    "        merge_features = torch.cat((sentences, visual, acoustic, pose, hand_r, hand_l), dim=2)\n",
    "        packed_sequence = pack_padded_sequence(merge_features, lengths)\n",
    "        _, (final_h, _) = self.biLSTM(packed_sequence)\n",
    "        h = final_h.view(batch_size, -1)\n",
    "        o = self.output(h)\n",
    "        #o = torch.cat((o[:,0].unsqueeze(dim=1), self.relu(o[:,1:])),axis=1)\n",
    "        if self.classification:\n",
    "            return self.softmax(o)\n",
    "        return o.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51ac604",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:35:29.120752Z",
     "start_time": "2022-05-23T13:35:29.108750Z"
    }
   },
   "outputs": [],
   "source": [
    "class EFLSTM(nn.Module):\n",
    "    def __init__(self, input_sizes, hidden_sizes, output_size, dropout_rate, n_layers=2, classification=False):\n",
    "        super(EFLSTM, self).__init__()\n",
    "        self.input_size = input_sizes\n",
    "        self.hidden_size = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.classification = classification\n",
    "\n",
    "        #self.embed = nn.Embedding(len(word_dict), input_sizes[0])\n",
    "\n",
    "        self.total_inputsize = sum(self.input_size[:3])\n",
    "        self.total_hiddensize = sum(self.hidden_size[:3])\n",
    "        self.biLSTM = nn.LSTM(\n",
    "            self.total_inputsize, self.total_hiddensize, bidirectional=True, num_layers=n_layers, dropout=dropout_rate)\n",
    "\n",
    "        if self.classification:\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.output = nn.Linear(self.total_hiddensize*2*n_layers, output_size)\n",
    "\n",
    "    def forward(self, sentences, visual, acoustic, pose, hand_r, hand_l, lengths):\n",
    "        batch_size = lengths.size(0)\n",
    "        #sentences = self.embed(sentences)\n",
    "        merge_features = torch.cat((sentences, visual, acoustic), dim=2)\n",
    "        packed_sequence = pack_padded_sequence(merge_features, lengths)\n",
    "        _, (final_h, _) = self.biLSTM(packed_sequence)\n",
    "        h = final_h.view(batch_size, -1)\n",
    "        o = self.output(h)\n",
    "        #o = torch.cat((o[:,0].unsqueeze(dim=1), self.relu(o[:,1:])),axis=1)\n",
    "        if self.classification:\n",
    "            return self.softmax(o)\n",
    "        return o.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4bb35e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:35:29.547758Z",
     "start_time": "2022-05-23T13:35:29.525162Z"
    }
   },
   "outputs": [],
   "source": [
    "# From https://github.com/Justin1904/CMU-MultimodalSDK-Tutorials/blob/master/tutorial_interactive.ipynb\n",
    "class LFLSTM(nn.Module):\n",
    "    def __init__(self, input_sizes, hidden_sizes, fc1_size, output_size, dropout_rate, classification=False):\n",
    "        super(LFLSTM, self).__init__()\n",
    "        self.input_size = input_sizes\n",
    "        self.hidden_size = hidden_sizes\n",
    "        self.fc1_size = fc1_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.classification = classification\n",
    "        # defining modules - two layer bidirectional LSTM with layer norm in between\n",
    "        # self.embed = nn.Embedding(len(word2id), input_sizes[0])\n",
    "        self.trnn1 = nn.LSTM(\n",
    "            input_sizes[0], hidden_sizes[0], bidirectional=True)\n",
    "        self.trnn2 = nn.LSTM(\n",
    "            2*hidden_sizes[0], hidden_sizes[0], bidirectional=True)\n",
    "\n",
    "        self.vrnn1 = nn.LSTM(\n",
    "            input_sizes[1], hidden_sizes[1], bidirectional=True)\n",
    "        self.vrnn2 = nn.LSTM(\n",
    "            2*hidden_sizes[1], hidden_sizes[1], bidirectional=True)\n",
    "\n",
    "        self.arnn1 = nn.LSTM(\n",
    "            input_sizes[2], hidden_sizes[2], bidirectional=True)\n",
    "        self.arnn2 = nn.LSTM(\n",
    "            2*hidden_sizes[2], hidden_sizes[2], bidirectional=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(sum(hidden_sizes)*4, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.tlayer_norm = nn.LayerNorm((hidden_sizes[0]*2,))\n",
    "        self.vlayer_norm = nn.LayerNorm((hidden_sizes[1]*2,))\n",
    "        self.alayer_norm = nn.LayerNorm((hidden_sizes[2]*2,))\n",
    "        self.bn = nn.BatchNorm1d(sum(hidden_sizes)*4)\n",
    "        self.relu = nn.ReLU()\n",
    "        if self.classification:\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def extract_features(self, sequence, lengths, rnn1, rnn2, layer_norm):\n",
    "        packed_sequence = pack_padded_sequence(sequence, lengths)\n",
    "        packed_h1, (final_h1, _) = rnn1(packed_sequence)\n",
    "        padded_h1, _ = pad_packed_sequence(packed_h1)\n",
    "        normed_h1 = layer_norm(padded_h1)\n",
    "        packed_normed_h1 = pack_padded_sequence(normed_h1, lengths)\n",
    "        _, (final_h2, _) = rnn2(packed_normed_h1)\n",
    "        return final_h1, final_h2\n",
    "\n",
    "    def fusion(self, sentences, visual, acoustic, lengths):\n",
    "        batch_size = lengths.size(0)\n",
    "        # In case no embeddings are used\n",
    "        # sentences = self.embed(sentences)\n",
    "        # extract features from text modality\n",
    "        final_h1t, final_h2t = self.extract_features(\n",
    "            sentences, lengths, self.trnn1, self.trnn2, self.tlayer_norm)\n",
    "\n",
    "        # extract features from visual modality\n",
    "        final_h1v, final_h2v = self.extract_features(\n",
    "            visual, lengths, self.vrnn1, self.vrnn2, self.vlayer_norm)\n",
    "\n",
    "        # extract features from acoustic modality\n",
    "        final_h1a, final_h2a = self.extract_features(\n",
    "            acoustic, lengths, self.arnn1, self.arnn2, self.alayer_norm)\n",
    "\n",
    "        # simple late fusion -- concatenation + normalization\n",
    "        h = torch.cat((final_h1t, final_h2t, final_h1v, final_h2v, final_h1a, final_h2a),\n",
    "                      dim=2).permute(1, 0, 2).contiguous().view(batch_size, -1)\n",
    "        return self.bn(h)\n",
    "\n",
    "    def forward(self, sentences, visual, acoustic, pose, hand_r, hand_l, lengths):\n",
    "        batch_size = lengths.size(0)\n",
    "        h = self.fusion(sentences, visual, acoustic, lengths)\n",
    "        h = self.fc1(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.relu(h)\n",
    "        o = self.fc2(h)\n",
    "        #o = torch.cat((o[:,0].unsqueeze(dim=1), self.relu(o[:,1:])),axis=1)\n",
    "        if self.classification:\n",
    "            return self.softmax(o)\n",
    "        return o.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e27c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:35:29.987748Z",
     "start_time": "2022-05-23T13:35:29.968991Z"
    }
   },
   "outputs": [],
   "source": [
    "# From https://github.com/Justin1904/CMU-MultimodalSDK-Tutorials/blob/master/tutorial_interactive.ipynb\n",
    "class LFLSTMP(nn.Module):\n",
    "    def __init__(self, input_sizes, hidden_sizes, fc1_size, output_size, dropout_rate, classification=False):\n",
    "        super(LFLSTMP, self).__init__()\n",
    "        self.input_size = input_sizes\n",
    "        self.hidden_size = hidden_sizes\n",
    "        self.fc1_size = fc1_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.classification = classification\n",
    "\n",
    "        # defining modules - two layer bidirectional LSTM with layer norm in between\n",
    "        # self.embed = nn.Embedding(len(word2id), input_sizes[0])\n",
    "        self.trnn1 = nn.LSTM(\n",
    "            input_sizes[0], hidden_sizes[0], bidirectional=True)\n",
    "        self.trnn2 = nn.LSTM(\n",
    "            2*hidden_sizes[0], hidden_sizes[0], bidirectional=True)\n",
    "\n",
    "        self.vrnn1 = nn.LSTM(\n",
    "            input_sizes[1], hidden_sizes[1], bidirectional=True)\n",
    "        self.vrnn2 = nn.LSTM(\n",
    "            2*hidden_sizes[1], hidden_sizes[1], bidirectional=True)\n",
    "\n",
    "        self.arnn1 = nn.LSTM(\n",
    "            input_sizes[2], hidden_sizes[2], bidirectional=True)\n",
    "        self.arnn2 = nn.LSTM(\n",
    "            2*hidden_sizes[2], hidden_sizes[2], bidirectional=True)\n",
    "\n",
    "        self.prnn1 = nn.LSTM(\n",
    "            input_sizes[3], hidden_sizes[3], bidirectional=True)\n",
    "        self.prnn2 = nn.LSTM(\n",
    "            2*hidden_sizes[3], hidden_sizes[3], bidirectional=True)\n",
    "        \n",
    "        self.hrrnn1 = nn.LSTM(\n",
    "            input_sizes[4], hidden_sizes[4], bidirectional=True)\n",
    "        self.hrrnn2 = nn.LSTM(\n",
    "            2*hidden_sizes[4], hidden_sizes[4], bidirectional=True)\n",
    "        \n",
    "        self.hlrnn1 = nn.LSTM(\n",
    "            input_sizes[5], hidden_sizes[5], bidirectional=True)\n",
    "        self.hlrnn2 = nn.LSTM(\n",
    "            2*hidden_sizes[5], hidden_sizes[5], bidirectional=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(sum(hidden_sizes)*4, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.tlayer_norm = nn.LayerNorm((hidden_sizes[0]*2,))\n",
    "        self.vlayer_norm = nn.LayerNorm((hidden_sizes[1]*2,))\n",
    "        self.alayer_norm = nn.LayerNorm((hidden_sizes[2]*2,))\n",
    "        self.player_norm = nn.LayerNorm((hidden_sizes[3]*2,))\n",
    "        self.hrlayer_norm = nn.LayerNorm((hidden_sizes[4]*2,))\n",
    "        self.hllayer_norm = nn.LayerNorm((hidden_sizes[5]*2,))\n",
    "        self.bn = nn.BatchNorm1d(sum(hidden_sizes)*4)\n",
    "        self.relu = nn.ReLU()\n",
    "        if self.classification:\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "    def extract_features(self, sequence, lengths, rnn1, rnn2, layer_norm):\n",
    "        packed_sequence = pack_padded_sequence(sequence, lengths)\n",
    "        packed_h1, (final_h1, _) = rnn1(packed_sequence)\n",
    "        padded_h1, _ = pad_packed_sequence(packed_h1)\n",
    "        normed_h1 = layer_norm(padded_h1)\n",
    "        packed_normed_h1 = pack_padded_sequence(normed_h1, lengths)\n",
    "        _, (final_h2, _) = rnn2(packed_normed_h1)\n",
    "        return final_h1, final_h2\n",
    "\n",
    "    def fusion(self, sentences, visual, acoustic, pose, hand_r, hand_l, lengths):\n",
    "        batch_size = lengths.size(0)\n",
    "        # We have features in sentences\n",
    "        #sentences = self.embed(sentences)\n",
    "        # extract features from text modality\n",
    "        final_h1t, final_h2t = self.extract_features(\n",
    "            sentences, lengths, self.trnn1, self.trnn2, self.tlayer_norm)\n",
    "\n",
    "        # extract features from visual modality\n",
    "        final_h1v, final_h2v = self.extract_features(\n",
    "            visual, lengths, self.vrnn1, self.vrnn2, self.vlayer_norm)\n",
    "\n",
    "        # extract features from acoustic modality\n",
    "        final_h1a, final_h2a = self.extract_features(\n",
    "            acoustic, lengths, self.arnn1, self.arnn2, self.alayer_norm)\n",
    "\n",
    "        # extract features from pose modality\n",
    "        final_h1p, final_h2p = self.extract_features(\n",
    "            pose, lengths, self.prnn1, self.prnn2, self.player_norm)\n",
    "        \n",
    "        final_h1hr, final_h2hr = self.extract_features(\n",
    "            hand_r, lengths, self.hrrnn1, self.hrrnn2, self.hrlayer_norm)\n",
    "        \n",
    "        final_h1hl, final_h2hl = self.extract_features(\n",
    "            hand_l, lengths, self.hlrnn1, self.hlrnn2, self.hllayer_norm)\n",
    "\n",
    "        # simple late fusion -- concatenation + normalization\n",
    "        h = torch.cat((final_h1t, final_h2t, final_h1v, final_h2v, final_h1a, final_h2a, final_h1p, final_h2p, final_h1hr, final_h2hr, final_h1hl, final_h2hl),\n",
    "                      dim=2).permute(1, 0, 2).contiguous().view(batch_size, -1)\n",
    "        return self.bn(h)\n",
    "\n",
    "    def forward(self, sentences, visual, acoustic, pose, hand_r, hand_l, lengths):\n",
    "        batch_size = lengths.size(0)\n",
    "        h = self.fusion(sentences, visual, acoustic, pose, hand_r, hand_l, lengths)\n",
    "        h = self.fc1(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.relu(h)\n",
    "        o = self.fc2(h)\n",
    "        #o = torch.cat((o[:,0].unsqueeze(dim=1), self.relu(o[:,1:])),axis=1)\n",
    "        if self.classification:\n",
    "            return self.softmax(o)\n",
    "        return o.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae48427",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:35:30.376193Z",
     "start_time": "2022-05-23T13:35:30.350692Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_set, dev_set, num_epochs, criterion_func, optimizer, patience_n=15, update_to_save=0.01, verbose=True):\n",
    "    # Train parameters\n",
    "    curr_patience = patience = patience_n\n",
    "    num_trials = 3\n",
    "    grad_clip_value = 1.0\n",
    "\n",
    "    if CUDA:\n",
    "        model.cuda()\n",
    "    criterion = criterion_func\n",
    "    best_valid_loss = float('inf')\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=1, gamma=0.1)\n",
    "    lr_scheduler.step()  # for some reason it seems the StepLR needs to be stepped once first\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for e in range(num_epochs):\n",
    "        model.train()\n",
    "        train_iter = train_set\n",
    "        train_loss = 0.0\n",
    "        for batch in train_iter:\n",
    "            model.zero_grad()\n",
    "            if POSE_FEATURES:\n",
    "                t, v, a, p, hr, hl, y, l = batch\n",
    "                batch_size = t.size(0)\n",
    "                if CUDA:\n",
    "                    t, v, a = t.cuda(), v.cuda(), a.cuda()\n",
    "                    p, hr, hl = p.cuda(), hr.cuda(), hl.cuda()\n",
    "                    y = y.cuda()\n",
    "                y_tilde = model(t, v, a, p, hr, hl, l)\n",
    "            else:\n",
    "                t, v, a, y, l = batch\n",
    "                batch_size = t.size(0)\n",
    "                if CUDA:\n",
    "                    t, v, a = t.cuda(), v.cuda(), a.cuda()\n",
    "                    y = y.cuda()\n",
    "                y_tilde = model(t, v, a, l)\n",
    "            loss = criterion(y_tilde, y)\n",
    "            loss.backward()\n",
    "            # Clip gradients to avoid vanishing gradients\n",
    "            torch.nn.utils.clip_grad_value_(\n",
    "                [param for param in model.parameters() if param.requires_grad], grad_clip_value)\n",
    "            # Update Weights\n",
    "            optimizer.step()\n",
    "            #train_iter.set_description(\n",
    "            #    f\"Epoch {e}/{MAX_EPOCH}, current batch loss: {round(loss.item()/batch_size, 4)}\")\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_set)\n",
    "        train_losses.append(train_loss)\n",
    "        if e % 5 == 0:\n",
    "            if verbose:\n",
    "                print(f\"E ({e+1}): Training loss: {round(train_loss, 4)}\")\n",
    "\n",
    "        # Perfom Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = 0.0\n",
    "            for batch in dev_set:\n",
    "                model.zero_grad()\n",
    "                if POSE_FEATURES:\n",
    "                    t, v, a, p, hr, hl, y, l = batch\n",
    "                    batch_size = t.size(0)\n",
    "                    if CUDA:\n",
    "                        t, v, a = t.cuda(), v.cuda(), a.cuda()\n",
    "                        p, hr, hl = p.cuda(), hr.cuda(), hl.cuda()\n",
    "                        y = y.cuda()\n",
    "                    y_tilde = model(t, v, a, p, hr, hl, l)\n",
    "                else:\n",
    "                    t, v, a, y, l = batch\n",
    "                    batch_size = t.size(0)\n",
    "                    if CUDA:\n",
    "                        t, v, a = t.cuda(), v.cuda(), a.cuda()\n",
    "                        y = y.cuda()\n",
    "                    y_tilde = model(t, v, a, l)\n",
    "                loss = criterion(y_tilde, y)\n",
    "                #print(\"Pred:\", y_tilde)\n",
    "                #print(\"True:\", y)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "        valid_loss = valid_loss/len(dev_set)\n",
    "        valid_losses.append(valid_loss)\n",
    "        if verbose:\n",
    "            print(f\"Validation loss: {round(valid_loss, 4)}\")\n",
    "        #print(f\"Current patience: {curr_patience}, current trial: {num_trials}.\")\n",
    "        improve = (valid_loss - best_valid_loss)\n",
    "        if improve == -float('inf'):\n",
    "            best_valid_loss = 0\n",
    "        # If improvement is above 1% of last loss\n",
    "        if verbose:\n",
    "            print(improve, (best_valid_loss * -update_to_save))\n",
    "        if improve < (best_valid_loss * -update_to_save) :\n",
    "            best_valid_loss = valid_loss\n",
    "            if verbose:\n",
    "                print(\"Found new best model on dev set! Saving current params\")\n",
    "            torch.save(model.state_dict(), 'model.std')\n",
    "            torch.save(optimizer.state_dict(), 'optim.std')\n",
    "            curr_patience = patience\n",
    "        else:\n",
    "            curr_patience -= 1\n",
    "            if curr_patience < 0:\n",
    "                if verbose:\n",
    "                    print(\"Running out of patience, loading previous best model.\")\n",
    "                num_trials -= 1\n",
    "                curr_patience = patience\n",
    "                model.load_state_dict(torch.load('model.std'))\n",
    "                optimizer.load_state_dict(torch.load('optim.std'))\n",
    "                lr_scheduler.step()\n",
    "                if verbose:\n",
    "                    print(f\"Current learning rate: {optimizer.state_dict()['param_groups'][0]['lr']}\")\n",
    "        if num_trials <= 0:\n",
    "            if verbose:\n",
    "                print(\"Running out of patience, early stopping.\")\n",
    "            break\n",
    "    return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f27b29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:35:30.639557Z",
     "start_time": "2022-05-23T13:35:30.616552Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def test_model_classification(model, test_loader, criterion_func):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.0\n",
    "        for batch in test_loader:\n",
    "            model.zero_grad()\n",
    "            if POSE_FEATURES:\n",
    "                t, v, a, p, hr, hl, y, l = batch\n",
    "                batch_size = t.size(0)\n",
    "                if CUDA:\n",
    "                    t, v, a = t.cuda(), v.cuda(), a.cuda()\n",
    "                    p, hr, hl = p.cuda(), hr.cuda(), hl.cuda()\n",
    "                    y = y.cuda()\n",
    "                y_tilde = model(t, v, a, p, hr, hl, l)\n",
    "            else:\n",
    "                t, v, a, y, l = batch\n",
    "                batch_size = t.size(0)\n",
    "                if CUDA:\n",
    "                    t, v, a = t.cuda(), v.cuda(), a.cuda()\n",
    "                    y = y.cuda()\n",
    "                y_tilde = model(t, v, a, l)\n",
    "            loss = criterion_func(y_tilde, y)\n",
    "            y_tilde_numpy = y_tilde.detach().cpu().numpy()\n",
    "            if len(y_tilde_numpy.shape) == 0:\n",
    "                y_pred += [y_tilde_numpy]\n",
    "            else:\n",
    "                y_pred += [e for e in y_tilde_numpy]\n",
    "            y_true += [e for e in y.detach().cpu().numpy()]\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred =y_pred.argmax(axis=1)\n",
    "\n",
    "    if len(y_pred.shape) > 1: \n",
    "        #y_pred[:,1:][y_pred[:,1:] < 0] = 0\n",
    "        f1_s = f1_score(y_true, y_pred, average='weighted')\n",
    "        acc_s = (y_true == y_pred).sum()/len(y_pred)\n",
    "        print(f\"Test set F1S is {f1_s}\")\n",
    "        #print(\"--Average Error per dimension--\")\n",
    "        #print(np.abs(y_true - y_pred).mean(axis=0))\n",
    "        return y_pred, y_true, (f1_s, acc_s)\n",
    "    else:\n",
    "        #y_pred[1:][y_pred[1:] < 0] = 0\n",
    "        #y_true = y_true[0]\n",
    "        f1_s = f1_score(y_true, y_pred, average='weighted')\n",
    "        acc_s = (y_true == y_pred).sum()/len(y_pred)\n",
    "        print(f\"Test set F1S is {f1_s}\")\n",
    "        #print(np.abs(y_true - y_pred))\n",
    "        return y_pred, y_true, (f1_s, acc_s)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faf8a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def test_model_regression(model, test_loader, criterion_func):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.0\n",
    "        for batch in test_loader:\n",
    "            model.zero_grad()\n",
    "            if POSE_FEATURES:\n",
    "                t, v, a, p, hr, hl, y, l = batch\n",
    "                batch_size = t.size(0)\n",
    "                if CUDA:\n",
    "                    t, v, a = t.cuda(), v.cuda(), a.cuda()\n",
    "                    p, hr, hl = p.cuda(), hr.cuda(), hl.cuda()\n",
    "                    y = y.cuda()\n",
    "                y_tilde = model(t, v, a, p, hr, hl, l)\n",
    "            else:\n",
    "                t, v, a, y, l = batch\n",
    "                batch_size = t.size(0)\n",
    "                if CUDA:\n",
    "                    t, v, a = t.cuda(), v.cuda(), a.cuda()\n",
    "                    y = y.cuda()\n",
    "                y_tilde = model(t, v, a, l)\n",
    "            loss = criterion_func(y_tilde, y)\n",
    "            y_tilde_numpy = y_tilde.detach().cpu().numpy()\n",
    "            if len(y_tilde_numpy.shape) == 0:\n",
    "                y_pred += [y_tilde_numpy]\n",
    "            else:\n",
    "                y_pred += [e for e in y_tilde_numpy]\n",
    "            y_true += [e for e in y.detach().cpu().numpy()]\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "\n",
    "    if len(y_pred.shape) > 1: \n",
    "        #y_pred[:,1:][y_pred[:,1:] < 0] = 0\n",
    "        #error_test = criterion_func(y_true, y_pred)\n",
    "        print(f\"Test set MAE is {test_loss}\")\n",
    "        #print(\"--Average Error per dimension--\")\n",
    "        #print(np.abs(y_true - y_pred).mean(axis=0))\n",
    "        return y_pred, y_true, test_loss#, np.abs(y_true - y_pred).mean(axis=0) \n",
    "    else:\n",
    "        #y_pred[1:][y_pred[1:] < 0] = 0\n",
    "        #y_true = y_true[0]\n",
    "        #error_test = criterion_func(y_true, y_pred)\n",
    "        print(f\"Test set MAE is {test_loss}\")\n",
    "        #print(np.abs(y_true - y_pred))\n",
    "        return y_pred, y_true, test_loss#, np.abs(y_true - y_pred)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15208e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LFLSTMP_run(train_loader, dev_loader, test_loader,\n",
    "    input_sizes, hidden_sizes, fc1_size, output_size, dropout, epoch_max, weight_decay, classification):\n",
    "\n",
    "    model = LFLSTMP(input_sizes, hidden_sizes, fc1_size, output_size, dropout, classification)\n",
    "    optimizer = Adam([param for param in model.parameters()\n",
    "                    if param.requires_grad], weight_decay=weight_decay)\n",
    "    if classification:\n",
    "        train_losses, valid_losses = train_model(model, train_loader, dev_loader, epoch_max, nn.CrossEntropyLoss(), optimizer, patience_n=25)\n",
    "    else:\n",
    "        train_losses, valid_losses = train_model(model, train_loader, dev_loader, epoch_max, nn.MSELoss(), optimizer, patience_n=25)\n",
    "    model.load_state_dict(torch.load('model.std'))\n",
    "    if classification:\n",
    "        results = test_model_classification(model, test_loader, nn.CrossEntropyLoss())\n",
    "    else:\n",
    "        results = test_model_regression(model, test_loader, nn.L1Loss())\n",
    "    return train_losses, valid_losses, results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0fa118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LFLSTM_run(train_loader, dev_loader, test_loader,\n",
    "    input_sizes, hidden_sizes, fc1_size, output_size, dropout, epoch_max, weight_decay, classification):\n",
    "    print(\"Performing Late Fusion LSTM with pose Run\")\n",
    "\n",
    "    model = LFLSTM(input_sizes, hidden_sizes, fc1_size, output_size, dropout, classification)\n",
    "    optimizer = Adam([param for param in model.parameters()\n",
    "                    if param.requires_grad], weight_decay=weight_decay)\n",
    "    if classification:\n",
    "        train_losses, valid_losses = train_model(model, train_loader, dev_loader, epoch_max, nn.CrossEntropyLoss(), optimizer, patience_n=25)\n",
    "    else:\n",
    "        train_losses, valid_losses = train_model(model, train_loader, dev_loader, epoch_max, nn.MSELoss(), optimizer, patience_n=25)\n",
    "    model.load_state_dict(torch.load('model.std'))\n",
    "    if classification:\n",
    "        results = test_model_classification(model, test_loader, nn.CrossEntropyLoss())\n",
    "    else:\n",
    "        results = test_model_regression(model, test_loader, nn.L1Loss())\n",
    "    return train_losses, valid_losses, results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0434c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LFLSTM_run(train_loader, dev_loader, test_loader,\n",
    "    input_sizes, hidden_sizes, fc1_size, output_size, dropout, epoch_max, weight_decay, classification):\n",
    "    print(\"Performing Late Fusion LSTM NO pose Run\")\n",
    "\n",
    "    model = LFLSTM(input_sizes, hidden_sizes, fc1_size, output_size, dropout, classification)\n",
    "    optimizer = Adam([param for param in model.parameters()\n",
    "                    if param.requires_grad], weight_decay=weight_decay)\n",
    "    if classification:\n",
    "        train_losses, valid_losses = train_model(model, train_loader, dev_loader, epoch_max, nn.CrossEntropyLoss(), optimizer, patience_n=25)\n",
    "    else:\n",
    "        train_losses, valid_losses = train_model(model, train_loader, dev_loader, epoch_max, nn.MSELoss(), optimizer, patience_n=25)\n",
    "    model.load_state_dict(torch.load('model.std'))\n",
    "    if classification:\n",
    "        results = test_model_classification(model, test_loader, nn.CrossEntropyLoss())\n",
    "    else:\n",
    "        results = test_model_regression(model, test_loader, nn.L1Loss())\n",
    "    return train_losses, valid_losses, results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fb0d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EFLSTMP_run(train_loader, dev_loader, test_loader,\n",
    "    input_sizes, hidden_sizes, output_size, dropout, epoch_max, lstm_dim, weight_decay, classification):\n",
    "    print(\"Performing Early Fusion LSTM with pose Run\")\n",
    "\n",
    "    model = EFLSTMP(input_sizes, hidden_sizes, output_size, dropout, n_layers=lstm_dim, classification=classification)\n",
    "    optimizer = Adam([param for param in model.parameters()\n",
    "                    if param.requires_grad], weight_decay=weight_decay)\n",
    "    if classification:\n",
    "        train_losses, valid_losses = train_model(model, train_loader, dev_loader, epoch_max, nn.CrossEntropyLoss(), optimizer, patience_n=25)\n",
    "    else:\n",
    "        train_losses, valid_losses = train_model(model, train_loader, dev_loader, epoch_max, nn.MSELoss(), optimizer, patience_n=25)\n",
    "    model.load_state_dict(torch.load('model.std'))\n",
    "    if classification:\n",
    "        results = test_model_classification(model, test_loader, nn.CrossEntropyLoss())\n",
    "    else:\n",
    "        results = test_model_regression(model, test_loader, nn.L1Loss())\n",
    "    return train_losses, valid_losses, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ba887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EFLSTM_run(train_loader, dev_loader, test_loader,\n",
    "    input_sizes, hidden_sizes, output_size, dropout, epoch_max, lstm_dim, weight_decay, classification):\n",
    "    print(\"Performing Early Fusion LSTM NO pose Run\")\n",
    "\n",
    "    model = EFLSTM(input_sizes, hidden_sizes, output_size, dropout, n_layers=lstm_dim, classification=classification)\n",
    "    optimizer = Adam([param for param in model.parameters()\n",
    "                    if param.requires_grad], weight_decay=weight_decay)\n",
    "    if classification:\n",
    "        train_losses, valid_losses = train_model(model, train_loader, dev_loader, epoch_max, nn.CrossEntropyLoss(), optimizer, patience_n=25)\n",
    "    else:\n",
    "        train_losses, valid_losses = train_model(model, train_loader, dev_loader, epoch_max, nn.MSELoss(), optimizer, patience_n=25)\n",
    "    model.load_state_dict(torch.load('model.std'))\n",
    "    if classification:\n",
    "        results = test_model_classification(model, test_loader, nn.CrossEntropyLoss())\n",
    "    else:\n",
    "        results = test_model_regression(model, test_loader, nn.L1Loss())\n",
    "    return train_losses, valid_losses, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7078ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LangLSTM_run(train_loader, dev_loader, test_loader,\n",
    "    input_sizes, hidden_sizes, output_size, dropout, epoch_max, lstm_dim, weight_decay, classification):\n",
    "    print(\"Performing Early Fusion LSTM NO pose Run\")\n",
    "\n",
    "    model = LangLSTM(input_sizes, hidden_sizes, output_size, dropout, n_layers=lstm_dim, classification=classification)\n",
    "    optimizer = Adam([param for param in model.parameters()\n",
    "                    if param.requires_grad], weight_decay=weight_decay)\n",
    "    if classification:\n",
    "        train_losses, valid_losses = train_model(model, train_loader, dev_loader, epoch_max, nn.CrossEntropyLoss(), optimizer, patience_n=25)\n",
    "    else:\n",
    "        train_losses, valid_losses = train_model(model, train_loader, dev_loader, epoch_max, nn.MSELoss(), optimizer, patience_n=25)\n",
    "    model.load_state_dict(torch.load('model.std'))\n",
    "    if classification:\n",
    "        results = test_model_classification(model, test_loader, nn.CrossEntropyLoss())\n",
    "    else:\n",
    "        results = test_model_regression(model, test_loader, nn.L1Loss())\n",
    "    return train_losses, valid_losses, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79311427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE OUT TESTING\n",
    "def leave_one_out(dataset, dict_results=dict(), save_plots=True, label_n=0, output_size=1,  num_classes=-1, classification_task=False, graph_path=\".\"):\n",
    "    all_videos = list(videos)\n",
    "\n",
    "    CUDA = torch.cuda.is_available()\n",
    "    print(\"CUDA: \", CUDA)\n",
    "    MAX_EPOCH = 1000\n",
    "\n",
    "    text_size = 300\n",
    "    visual_size = 35\n",
    "    acoustic_size = 74\n",
    "    pose_size = 25\n",
    "    hand_r_size = 21\n",
    "    hand_l_size = 21\n",
    "    num_classes = num_classes\n",
    "    output_size = output_size\n",
    "    classification = classification_task\n",
    "    dropout_lflstm = 0.15\n",
    "    dropout_eflstm = 0.05\n",
    "    n_layer_eflstm = 2\n",
    "    weight_decay = 0.1\n",
    "\n",
    "    dict_results[\"videos\"] = all_videos\n",
    "    for test_i in range(0, len(all_videos)):\n",
    "        test_v = [all_videos[test_i]]\n",
    "        for dev_i in range(0, len(all_videos)):\n",
    "            if dev_i != test_i:\n",
    "                print(\"Dev i:\", dev_i, \" || Test i: \", test_i,)\n",
    "                dev_v = [all_videos[dev_i]]\n",
    "                train_v = [v for v in all_videos if v not in test_v and v not in dev_v]\n",
    "                train_s, dev_s, test_s = get_train_dev_test_splits(dataset, train_v, test_v, dev_v, label_n=label_n, n_classes=num_classes)\n",
    "                if len(dev_s) == 0 or len(test_s) == 0:\n",
    "                    print(\"WARNING 0 SEGMENTS in TEST OR DEV, skipping...\")\n",
    "                    continue\n",
    "                if classification_task:\n",
    "                    train_loader = DataLoader(train_s, shuffle=False, batch_size=32, collate_fn=multi_collate_class)\n",
    "                    dev_loader = DataLoader(dev_s, shuffle=False, batch_size=32, collate_fn=multi_collate_class)\n",
    "                    test_loader = DataLoader(test_s, shuffle=False, batch_size=32, collate_fn=multi_collate_class)\n",
    "                else:\n",
    "                    train_loader = DataLoader(train_s, shuffle=False, batch_size=32, collate_fn=multi_collate_reg)\n",
    "                    dev_loader = DataLoader(dev_s, shuffle=False, batch_size=32, collate_fn=multi_collate_reg)\n",
    "                    test_loader = DataLoader(test_s, shuffle=False, batch_size=32, collate_fn=multi_collate_reg)\n",
    "                print(np.array(train_s).shape, np.array(dev_s).shape, np.array(test_s).shape)\n",
    "                lflstmp_settings = ( [text_size, visual_size, acoustic_size, pose_size, hand_r_size, hand_l_size], \n",
    "                                    [int(text_size * 1.25), int(visual_size * 1.25), int(acoustic_size * 1.25), int(pose_size*1.), int(hand_r_size*1.), int(hand_l_size*1.)],\n",
    "                                    sum([int(text_size * 1.25), int(visual_size * 1.25), int(acoustic_size * 1.25), int(pose_size*1.), int(hand_r_size*1.), int(hand_l_size*1.)]) // 2,\n",
    "                                    output_size,\n",
    "                                    dropout_lflstm,\n",
    "                                    MAX_EPOCH,\n",
    "                                    weight_decay,\n",
    "                                    classification\n",
    "                                )\n",
    "\n",
    "                dict_results[(\"LFLSTMP\",test_i,dev_i)] = LFLSTMP_run(train_loader, dev_loader, test_loader, *lflstmp_settings)\n",
    "                train_losses, valid_losses, _ = dict_results[(\"LFLSTMP\",test_i,dev_i)]\n",
    "                if save_plots:\n",
    "                    plt.plot(np.arange(0, len(train_losses)), train_losses, label=\"Train Loss\")\n",
    "                    plt.plot(np.arange(0, len(valid_losses)), valid_losses, label=\"Dev Loss\")\n",
    "                    plt.legend()\n",
    "                    plt.title(f\"Training LFLSTM w/P, Dev: {dev_i} | Test: {test_i}\")\n",
    "                    plt.xlabel(\"N Epochs\")\n",
    "                    plt.ylabel(\"MSE\")\n",
    "                    plt.savefig(os.path.join(graph_path, \"LFLSTMP\", f\"LFLSTMP_t{test_i}_d{dev_i}.png\"))\n",
    "                    plt.clf()\n",
    "\n",
    "                lflstm_settings = ( [text_size, visual_size, acoustic_size,], \n",
    "                                    [int(text_size * 1.5), int(visual_size * 1.5), int(acoustic_size * 1.5),],\n",
    "                                    sum([int(text_size * 1.5), int(visual_size * 1.5), int(acoustic_size * 1.5),]) // 2,\n",
    "                                    output_size,\n",
    "                                    dropout_lflstm,\n",
    "                                    MAX_EPOCH,\n",
    "                                    weight_decay,\n",
    "                                    classification\n",
    "                                )\n",
    "\n",
    "                dict_results[(\"LFLSTM\",test_i,dev_i)] = LFLSTM_run(train_loader, dev_loader, test_loader, *lflstm_settings)\n",
    "                train_losses, valid_losses, _ = dict_results[(\"LFLSTM\",test_i,dev_i)]\n",
    "                if save_plots:\n",
    "                    plt.plot(np.arange(0, len(train_losses)), train_losses, label=\"Train Loss\")\n",
    "                    plt.plot(np.arange(0, len(valid_losses)), valid_losses, label=\"Dev Loss\")\n",
    "                    plt.legend()\n",
    "                    plt.title(f\"Training LFLSTM, Dev: {dev_i} | Test: {test_i}\")\n",
    "                    plt.xlabel(\"N Epochs\")\n",
    "                    plt.ylabel(\"MSE\")\n",
    "                    plt.savefig(os.path.join(graph_path, \"LFLSTM\", f\"LFLSTM_t{test_i}_d{dev_i}.png\"))\n",
    "                    plt.clf()\n",
    "\n",
    "                eflstmp_settings = ([text_size, visual_size, acoustic_size, pose_size, hand_r_size, hand_l_size], \n",
    "                                    [int(text_size * 1.25), int(visual_size * 1.25), int(acoustic_size * 1.25), int(pose_size*1.), int(hand_r_size*1.), int(hand_l_size*1.)],\n",
    "                                    output_size,\n",
    "                                    dropout_eflstm,\n",
    "                                    MAX_EPOCH,\n",
    "                                    n_layer_eflstm,\n",
    "                                    weight_decay,\n",
    "                                    classification\n",
    "                                )\n",
    "\n",
    "                dict_results[(\"EFLSTMP\",test_i,dev_i)] = EFLSTMP_run(train_loader, dev_loader, test_loader, *eflstmp_settings)\n",
    "                train_losses, valid_losses, _ = dict_results[(\"EFLSTMP\",test_i,dev_i)]\n",
    "                if save_plots:\n",
    "                    plt.plot(np.arange(0, len(train_losses)), train_losses, label=\"Train Loss\")\n",
    "                    plt.plot(np.arange(0, len(valid_losses)), valid_losses, label=\"Dev Loss\")\n",
    "                    plt.legend()\n",
    "                    plt.title(f\"Training EFLSTMP w/P, Dev: {dev_i} | Test: {test_i}\")\n",
    "                    plt.xlabel(\"N Epochs\")\n",
    "                    plt.ylabel(\"MSE\")\n",
    "                    plt.savefig(os.path.join(graph_path, \"EFLSTMP\", f\"EFLSTMP_t{test_i}_d{dev_i}.png\"))\n",
    "                    plt.clf()\n",
    "\n",
    "                eflstm_settings = ( [text_size, visual_size, acoustic_size,], \n",
    "                                    [int(text_size * 1.5), int(visual_size * 1.5), int(acoustic_size * 1.5),],\n",
    "                                    output_size,\n",
    "                                    dropout_eflstm,\n",
    "                                    MAX_EPOCH,\n",
    "                                    n_layer_eflstm,\n",
    "                                    weight_decay,\n",
    "                                    classification\n",
    "                                )\n",
    "\n",
    "                dict_results[(\"EFLSTM\",test_i,dev_i)] = EFLSTM_run(train_loader, dev_loader, test_loader, *eflstm_settings)\n",
    "                train_losses, valid_losses, _ = dict_results[(\"EFLSTM\",test_i,dev_i)]\n",
    "                if save_plots:\n",
    "                    plt.plot(np.arange(0, len(train_losses)), train_losses, label=\"Train Loss\")\n",
    "                    plt.plot(np.arange(0, len(valid_losses)), valid_losses, label=\"Dev Loss\")\n",
    "                    plt.legend()\n",
    "                    plt.title(f\"Training EFLSTM, Dev: {dev_i} | Test: {test_i}\")\n",
    "                    plt.xlabel(\"N Epochs\")\n",
    "                    plt.ylabel(\"MSE\")\n",
    "                    plt.savefig(os.path.join(graph_path, \"EFLSTM\", f\"EFLSTM_t{test_i}_d{dev_i}.png\"))\n",
    "                    plt.clf()\n",
    "                \n",
    "    return dict_results                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8679c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saved_results = dict()\n",
    "#CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbaddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_dictionary_top10_mae = leave_one_out(dataset, saved_results, True, label_n=0, output_size=1,\n",
    "#                                 num_classes=-1, classification_task=False, graph_path=os.path.join(\"train_graphs\",\"Sentiment_MSE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add29795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def test_models(dataset, saved_results, save_graph, label_n=0, output_size=1,\n",
    "                                 num_classes=-1, classification_task=False, graph_path=os.path.join(\"train_graphs\",\"Sentiment_MSE\"),\n",
    "                                 dictionary_name=\"\"):\n",
    "    resulting_dict = leave_one_out(dataset, saved_results, save_plots=save_graph, label_n=label_n, output_size=output_size,\n",
    "                                    num_classes=num_classes, classification_task=classification_task, graph_path=graph_path)\n",
    "    with open(dictionary_name+\".pkl\",\"wb\") as f:\n",
    "        pickle.dump(resulting_dict,f)\n",
    "        \n",
    "    if classification_task:\n",
    "        LFLSTMP_average_acc = [resulting_dict[key][2][2][1] for key in resulting_dict.keys() if key[0] == \"LFLSTMP\"]\n",
    "        LFLSTM_average_acc = [resulting_dict[key][2][2][1] for key in resulting_dict.keys() if key[0] == \"LFLSTM\"]\n",
    "        EFLSTM_average_acc = [resulting_dict[key][2][2][1] for key in resulting_dict.keys() if key[0] == \"EFLSTMP\"]\n",
    "        EFLSTMNP_average_acc = [resulting_dict[key][2][2][1] for key in resulting_dict.keys() if key[0] == \"EFLSTM\"]\n",
    "        LFLSTMP_average_f1 = [resulting_dict[key][2][2][0] for key in resulting_dict.keys() if key[0] == \"LFLSTMP\"]\n",
    "        LFLSTM_average_f1 = [resulting_dict[key][2][2][0] for key in resulting_dict.keys() if key[0] == \"LFLSTM\"]\n",
    "        EFLSTM_average_f1 = [resulting_dict[key][2][2][0] for key in resulting_dict.keys() if key[0] == \"EFLSTMP\"]\n",
    "        EFLSTMNP_average_f1 = [resulting_dict[key][2][2][0] for key in resulting_dict.keys() if key[0] == \"EFLSTM\"]\n",
    "        print(dictionary_name, \"RESULTS: \")\n",
    "        print(\"Average Acc:\")\n",
    "        print(np.array(LFLSTMP_average_acc).mean(),np.array(LFLSTM_average_acc).mean(),\n",
    "        np.array(EFLSTM_average_acc).mean(),np.array(EFLSTMNP_average_acc).mean())\n",
    "        print(\"Average F1:\")\n",
    "        print(np.array(LFLSTMP_average_f1).mean(),np.array(LFLSTM_average_f1).mean(),\n",
    "        np.array(EFLSTM_average_f1).mean(),np.array(EFLSTMNP_average_f1).mean())\n",
    "    else:\n",
    "        LFLSTMP_average_mae = [resulting_dict[key][2][2] for key in resulting_dict.keys() if key[0] == \"LFLSTMP\"]\n",
    "        LFLSTM_average_mae = [resulting_dict[key][2][2] for key in resulting_dict.keys() if key[0] == \"LFLSTM\"]\n",
    "        EFLSTM_average_mae = [resulting_dict[key][2][2] for key in resulting_dict.keys() if key[0] == \"EFLSTMP\"]\n",
    "        EFLSTMNP_average_mae = [resulting_dict[key][2][2] for key in resulting_dict.keys() if key[0] == \"EFLSTM\"]\n",
    "        print(dictionary_name, \"RESULTS: \")\n",
    "        print(\"MAE: \")\n",
    "        print(np.array(LFLSTMP_average_mae).mean(),np.array(LFLSTM_average_mae).mean(),\n",
    "        np.array(EFLSTM_average_mae).mean(),np.array(EFLSTMNP_average_mae).mean())\n",
    "    return resulting_dict\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_dictionary = dict()\n",
    "results_dictionary_top10_h4c = test_models(dataset, saved_dictionary, save_graph=True, label_n=1, output_size=4, \n",
    "                                    num_classes=4, classification_task=True, graph_path=os.path.join(\"train_graphs\",\"Happy_4c\"),\n",
    "                                    dictionary_name=\"results_dictionary_top10_h4c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce71dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LFLSTMP_average_acc = [saved_dictionary[key][2][2][1] for key in saved_dictionary.keys() if key[0] == \"LFLSTMP\"]\n",
    "LFLSTM_average_acc = [saved_dictionary[key][2][2][1] for key in saved_dictionary.keys() if key[0] == \"LFLSTM\"]\n",
    "EFLSTM_average_acc = [saved_dictionary[key][2][2][1] for key in saved_dictionary.keys() if key[0] == \"EFLSTMP\"]\n",
    "EFLSTMNP_average_acc = [saved_dictionary[key][2][2][1] for key in saved_dictionary.keys() if key[0] == \"EFLSTM\"]\n",
    "LFLSTMP_average_f1 = [saved_dictionary[key][2][2][0] for key in saved_dictionary.keys() if key[0] == \"LFLSTMP\"]\n",
    "LFLSTM_average_f1 = [saved_dictionary[key][2][2][0] for key in saved_dictionary.keys() if key[0] == \"LFLSTM\"]\n",
    "EFLSTM_average_f1 = [saved_dictionary[key][2][2][0] for key in saved_dictionary.keys() if key[0] == \"EFLSTMP\"]\n",
    "EFLSTMNP_average_f1 = [saved_dictionary[key][2][2][0] for key in saved_dictionary.keys() if key[0] == \"EFLSTM\"]\n",
    "print(\"Average Acc:\")\n",
    "print(\"LFLSTMP: \", np.array(LFLSTMP_average_acc).mean())\n",
    "print(\"LFLSTM: \", np.array(LFLSTM_average_acc).mean())\n",
    "print(\"EFLSTMP: \", np.array(EFLSTMNP_average_acc).mean())\n",
    "print(\"EFLSTM: \", np.array(EFLSTM_average_acc).mean())\n",
    "print(\"Average F1:\")\n",
    "print(\"LFLSTMP: \", np.array(LFLSTMP_average_f1).mean())\n",
    "print(\"LFLSTM: \", np.array(LFLSTM_average_f1).mean())\n",
    "print(\"EFLSTMP: \", np.array(EFLSTMNP_average_f1).mean())\n",
    "print(\"EFLSTM: \", np.array(EFLSTM_average_f1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce918a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: 'sentiment',\n",
    "# 1:  'happy',\n",
    "# 2:  'sad',\n",
    "# 3:  'anger',\n",
    "# 4:  'surprise',\n",
    "# 5:  'disgust',\n",
    "# 6:  'fear'],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa0ecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_dictionary = dict()\n",
    "results_dictionary_top10_h4c = test_models(dataset, saved_dictionary, save_graph=True, label_n=2, output_size=4, \n",
    "                                    num_classes=4, classification_task=True, graph_path=os.path.join(\"train_graphs\",\"Sad_4c\"),\n",
    "                                    dictionary_name=\"results_dictionary_top10_s4c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2763f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LFLSTMP_average_acc = [saved_dictionary[key][2][2][1] for key in saved_dictionary.keys() if key[0] == \"LFLSTMP\"]\n",
    "LFLSTM_average_acc = [saved_dictionary[key][2][2][1] for key in saved_dictionary.keys() if key[0] == \"LFLSTM\"]\n",
    "EFLSTM_average_acc = [saved_dictionary[key][2][2][1] for key in saved_dictionary.keys() if key[0] == \"EFLSTMP\"]\n",
    "EFLSTMNP_average_acc = [saved_dictionary[key][2][2][1] for key in saved_dictionary.keys() if key[0] == \"EFLSTM\"]\n",
    "LFLSTMP_average_f1 = [saved_dictionary[key][2][2][0] for key in saved_dictionary.keys() if key[0] == \"LFLSTMP\"]\n",
    "LFLSTM_average_f1 = [saved_dictionary[key][2][2][0] for key in saved_dictionary.keys() if key[0] == \"LFLSTM\"]\n",
    "EFLSTM_average_f1 = [saved_dictionary[key][2][2][0] for key in saved_dictionary.keys() if key[0] == \"EFLSTMP\"]\n",
    "EFLSTMNP_average_f1 = [saved_dictionary[key][2][2][0] for key in saved_dictionary.keys() if key[0] == \"EFLSTM\"]\n",
    "print(\"Average Acc:\")\n",
    "print(\"LFLSTMP: \", np.array(LFLSTMP_average_acc).mean())\n",
    "print(\"LFLSTM: \", np.array(LFLSTM_average_acc).mean())\n",
    "print(\"EFLSTMP: \", np.array(EFLSTMNP_average_acc).mean())\n",
    "print(\"EFLSTM: \", np.array(EFLSTM_average_acc).mean())\n",
    "print(\"Average F1:\")\n",
    "print(\"LFLSTMP: \", np.array(LFLSTMP_average_f1).mean())\n",
    "print(\"LFLSTM: \", np.array(LFLSTM_average_f1).mean())\n",
    "print(\"EFLSTMP: \", np.array(EFLSTMNP_average_f1).mean())\n",
    "print(\"EFLSTM: \", np.array(EFLSTM_average_f1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b71ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: 'sentiment',\n",
    "# 1:  'happy',\n",
    "# 2:  'sad',\n",
    "# 3:  'anger',\n",
    "# 4:  'surprise',\n",
    "# 5:  'disgust',\n",
    "# 6:  'fear'],\n",
    "saved_dictionary = dict()\n",
    "results_dictionary_top10_a7c = test_models(dataset, saved_dictionary, save_graph=True, label_n=3, output_size=4, \n",
    "                                    num_classes=4, classification_task=True, graph_path=os.path.join(\"train_graphs\",\"Anger_4c\"),\n",
    "                                    dictionary_name=\"results_dictionary_top10_a4c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933fabad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: 'sentiment',\n",
    "# 1:  'happy',\n",
    "# 2:  'sad',\n",
    "# 3:  'anger',\n",
    "# 4:  'surprise',\n",
    "# 5:  'disgust',\n",
    "# 6:  'fear'],\n",
    "saved_dictionary = dict()\n",
    "results_dictionary_top10_f7c = test_models(dataset, saved_dictionary, save_graph=True, label_n=6, output_size=4, \n",
    "                                    num_classes=4, classification_task=True, graph_path=os.path.join(\"train_graphs\",\"Fear_4c\"),\n",
    "                                    dictionary_name=\"results_dictionary_top10_f4c\")\n",
    "LFLSTMP_average_acc = [saved_dictionary[key][2][2][1] for key in saved_dictionary.keys() if key[0] == \"LFLSTMP\"]\n",
    "LFLSTM_average_acc = [saved_dictionary[key][2][2][1] for key in saved_dictionary.keys() if key[0] == \"LFLSTM\"]\n",
    "EFLSTM_average_acc = [saved_dictionary[key][2][2][1] for key in saved_dictionary.keys() if key[0] == \"EFLSTMP\"]\n",
    "EFLSTMNP_average_acc = [saved_dictionary[key][2][2][1] for key in saved_dictionary.keys() if key[0] == \"EFLSTM\"]\n",
    "LFLSTMP_average_f1 = [saved_dictionary[key][2][2][0] for key in saved_dictionary.keys() if key[0] == \"LFLSTMP\"]\n",
    "LFLSTM_average_f1 = [saved_dictionary[key][2][2][0] for key in saved_dictionary.keys() if key[0] == \"LFLSTM\"]\n",
    "EFLSTM_average_f1 = [saved_dictionary[key][2][2][0] for key in saved_dictionary.keys() if key[0] == \"EFLSTMP\"]\n",
    "EFLSTMNP_average_f1 = [saved_dictionary[key][2][2][0] for key in saved_dictionary.keys() if key[0] == \"EFLSTM\"]\n",
    "print(\"Average Acc:\")\n",
    "print(\"LFLSTMP: \", np.array(LFLSTMP_average_acc).mean())\n",
    "print(\"LFLSTM: \", np.array(LFLSTM_average_acc).mean())\n",
    "print(\"EFLSTMP: \", np.array(EFLSTMNP_average_acc).mean())\n",
    "print(\"EFLSTM: \", np.array(EFLSTM_average_acc).mean())\n",
    "print(\"Average F1:\")\n",
    "print(\"LFLSTMP: \", np.array(LFLSTMP_average_f1).mean())\n",
    "print(\"LFLSTM: \", np.array(LFLSTM_average_f1).mean())\n",
    "print(\"EFLSTMP: \", np.array(EFLSTMNP_average_f1).mean())\n",
    "print(\"EFLSTM: \", np.array(EFLSTM_average_f1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb7d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: 'sentiment',\n",
    "# 1:  'happy',\n",
    "# 2:  'sad',\n",
    "# 3:  'anger',\n",
    "# 4:  'surprise',\n",
    "# 5:  'disgust',\n",
    "# 6:  'fear'],\n",
    "saved_dictionary = dict()\n",
    "results_dictionary_top10_d7c = test_models(dataset, saved_dictionary, save_graph=True, label_n=5, output_size=4, \n",
    "                                    num_classes=4, classification_task=True, graph_path=os.path.join(\"train_graphs\",\"Disgust_4c\"),\n",
    "                                    dictionary_name=\"results_dictionary_top10_d4c\")\n",
    "LFLSTMP_average_acc = [saved_dictionary[key][2][2][1] for key in saved_dictionary.keys() if key[0] == \"LFLSTMP\"]\n",
    "LFLSTM_average_acc = [saved_dictionary[key][2][2][1] for key in saved_dictionary.keys() if key[0] == \"LFLSTM\"]\n",
    "EFLSTM_average_acc = [saved_dictionary[key][2][2][1] for key in saved_dictionary.keys() if key[0] == \"EFLSTMP\"]\n",
    "EFLSTMNP_average_acc = [saved_dictionary[key][2][2][1] for key in saved_dictionary.keys() if key[0] == \"EFLSTM\"]\n",
    "LFLSTMP_average_f1 = [saved_dictionary[key][2][2][0] for key in saved_dictionary.keys() if key[0] == \"LFLSTMP\"]\n",
    "LFLSTM_average_f1 = [saved_dictionary[key][2][2][0] for key in saved_dictionary.keys() if key[0] == \"LFLSTM\"]\n",
    "EFLSTM_average_f1 = [saved_dictionary[key][2][2][0] for key in saved_dictionary.keys() if key[0] == \"EFLSTMP\"]\n",
    "EFLSTMNP_average_f1 = [saved_dictionary[key][2][2][0] for key in saved_dictionary.keys() if key[0] == \"EFLSTM\"]\n",
    "print(\"Average Acc:\")\n",
    "print(\"LFLSTMP: \", np.array(LFLSTMP_average_acc).mean())\n",
    "print(\"LFLSTM: \", np.array(LFLSTM_average_acc).mean())\n",
    "print(\"EFLSTMP: \", np.array(EFLSTMNP_average_acc).mean())\n",
    "print(\"EFLSTM: \", np.array(EFLSTM_average_acc).mean())\n",
    "print(\"Average F1:\")\n",
    "print(\"LFLSTMP: \", np.array(LFLSTMP_average_f1).mean())\n",
    "print(\"LFLSTM: \", np.array(LFLSTM_average_f1).mean())\n",
    "print(\"EFLSTMP: \", np.array(EFLSTMNP_average_f1).mean())\n",
    "print(\"EFLSTM: \", np.array(EFLSTM_average_f1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340bd1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: 'sentiment',\n",
    "# 1:  'happy',\n",
    "# 2:  'sad',\n",
    "# 3:  'anger',\n",
    "# 4:  'surprise',\n",
    "# 5:  'disgust',\n",
    "# 6:  'fear'],\n",
    "saved_dictionary = dict()\n",
    "results_dictionary_top10_sur7c = test_models(dataset, saved_dictionary, save_graph=True, label_n=4, output_size=4, \n",
    "                                    num_classes=4, classification_task=True, graph_path=os.path.join(\"train_graphs\",\"Surprise_4c\"),\n",
    "                                    dictionary_name=\"results_dictionary_top10_surprise4c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a13f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_dictionary_test = None\n",
    "with open(\"results_dictionary_top10_mae.pkl\",'rb') as f:\n",
    "    saved_dictionary_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e4d367",
   "metadata": {},
   "outputs": [],
   "source": [
    "LFLSTMP_average_mae = [saved_dictionary_test[key][2][2] for key in saved_dictionary_test.keys() if key[0] == \"LFLSTMP\"]\n",
    "LFLSTM_average_mae = [saved_dictionary_test[key][2][2] for key in saved_dictionary_test.keys() if key[0] == \"LFLSTM\"]\n",
    "EFLSTM_average_mae = [saved_dictionary_test[key][2][2] for key in saved_dictionary_test.keys() if key[0] == \"EFLSTMP\"]\n",
    "EFLSTMNP_average_mae = [saved_dictionary_test[key][2][2] for key in saved_dictionary_test.keys() if key[0] == \"EFLSTM\"]\n",
    "print(\"MAE: \")\n",
    "print(np.array(LFLSTMP_average_mae).mean(),np.array(LFLSTM_average_mae).mean(),\n",
    "np.array(EFLSTM_average_mae).mean(),np.array(EFLSTMNP_average_mae).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bca360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(LFLSTMP_average_acc), np.mean(LFLSTM_average_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae5ccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"results_dictionary_top10_mae.pkl\",\"wb\") as f:\n",
    "    pickle.dump(results_dictionary_top10_mae,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205f8f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "LFLSTMP_average_mse = [results_dictionary_top10[key][2][2] for key in results_dictionary_top10.keys() if key[0] == \"LFLSTMP\"]\n",
    "LFLSTM_average_mse = [results_dictionary_top10[key][2][2] for key in results_dictionary_top10.keys() if key[0] == \"LFLSTM\"]\n",
    "EFLSTM_average_mse = [results_dictionary_top10[key][2][2] for key in results_dictionary_top10.keys() if key[0] == \"EFLSTMP\"]\n",
    "EFLSTMNP_average_mse = [results_dictionary_top10[key][2][2] for key in results_dictionary_top10.keys() if key[0] == \"EFLSTM\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90374cd5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458634e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(LFLSTMP_average_mse).mean(),np.array(LFLSTM_average_mse).mean(),np.array(EFLSTM_average_mse).mean(),np.array(EFLSTMNP_average_mse).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81234d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LFLSTMP_average_error_dim = [results_dictionary_top10[key][2][3] for key in results_dictionary_top10.keys() if key[0] == \"LFLSTMP\"]\n",
    "LFLSTM_average_error_dim  = [results_dictionary_top10[key][2][3] for key in results_dictionary_top10.keys() if key[0] == \"LFLSTM\"]\n",
    "EFLSTM_average_error_dim  = [results_dictionary_top10[key][2][3] for key in results_dictionary_top10.keys() if key[0] == \"EFLSTMP\"]\n",
    "EFLSTMNP_average_error_dim = [results_dictionary_top10[key][2][3] for key in results_dictionary_top10.keys() if key[0] == \"EFLSTM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c253b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(LFLSTMP_average_error_dim).mean(axis=0),np.array(LFLSTM_average_error_dim).mean(axis=0),np.array(EFLSTM_average_error_dim).mean(axis=0),np.array(EFLSTMNP_average_error_dim).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af265ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431e12e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:36:11.135584Z",
     "start_time": "2022-05-23T13:35:31.051746Z"
    }
   },
   "outputs": [],
   "source": [
    "CUDA = False#torch.cuda.is_available()\n",
    "print(\"CUDA: \", CUDA)\n",
    "MAX_EPOCH = 1000\n",
    "\n",
    "text_size = 300\n",
    "visual_size = 35\n",
    "acoustic_size = 74\n",
    "pose_size = 25\n",
    "hand_r_size = 21\n",
    "hand_l_size = 21\n",
    "\n",
    "# define some model settings and hyper-parameters\n",
    "input_sizes = [text_size, visual_size, acoustic_size, pose_size, hand_r_size, hand_l_size]\n",
    "hidden_sizes = [int(text_size * 1.25), int(visual_size * 1.25),\n",
    "                int(acoustic_size * 1.25), int(pose_size*1.), int(hand_r_size*1.), int(hand_l_size*1.)]\n",
    "fc1_size = sum(hidden_sizes) // 2\n",
    "dropout = 0.15\n",
    "output_size = 7\n",
    "weight_decay = 0.1\n",
    "\n",
    "\n",
    "model = LFLSTMP(input_sizes, hidden_sizes, fc1_size, output_size, dropout, True)\n",
    "\n",
    "optimizer = Adam([param for param in model.parameters()\n",
    "                 if param.requires_grad], weight_decay=weight_decay)\n",
    "\n",
    "train_losses, valid_losses = train_model(model, train_loader, dev_loader, MAX_EPOCH, nn.CrossEntropyLoss(), optimizer, patience_n=25, update_to_save=0.05)\n",
    "model.load_state_dict(torch.load('model.std'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b761cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test_model_classification(model, test_loader, nn.CrossEntropyLoss())\n",
    "# 1.18910813331604\n",
    "# [1.4783462  0.07662236 1.4068763  1.1306256  0.02685845 0.5496703 0.36040738]\n",
    "# [1 2 2 0 0]\n",
    "# [[0.24364449 0.25448358 0.24080864 0.26106322]\n",
    "#  [0.23234913 0.25409198 0.25244874 0.2611101 ]\n",
    "#  [0.22957    0.25405973 0.2547654  0.2616048 ]\n",
    "#  [0.22653975 0.27216277 0.24622498 0.25507247]\n",
    "#  [0.23375641 0.27675375 0.23467144 0.25481838]]\n",
    "# [3 3 3 1 1]\n",
    "# Test set F1S is 0.0\n",
    "# [Sentiment, Happiness, Sadness, Anger, Fear, Disgust, Surprise]\n",
    "# True (n=5):  [1 2 2 0 0]\n",
    "# Pred (n=5):  [3 3 3 1 1]\n",
    "# --Average Error per dimension--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c542e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:36:11.322537Z",
     "start_time": "2022-05-23T13:36:11.139584Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0, len(train_losses)), train_losses, label=\"Train Loss\")\n",
    "plt.plot(np.arange(0, len(valid_losses)), valid_losses, label=\"Dev Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f372b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:36:41.225457Z",
     "start_time": "2022-05-23T13:36:11.326538Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "CUDA = False #torch.cuda.is_available()\n",
    "print(\"CUDA: \", CUDA)\n",
    "MAX_EPOCH = 1000\n",
    "\n",
    "text_size = 300\n",
    "visual_size = 35\n",
    "acoustic_size = 74\n",
    "pose_size = 25\n",
    "hand_r_size = 21\n",
    "hand_l_size = 21\n",
    "\n",
    "# define some model settings and hyper-parameters\n",
    "input_sizes = [text_size, visual_size, acoustic_size]#, pose_size, hand_r_size, hand_l_size]\n",
    "hidden_sizes = [int(text_size * 1.5), int(visual_size * 1.5),\n",
    "                int(acoustic_size * 1.5)]#, int(pose_size*1.2), int(hand_r_size*1.2), int(hand_l_size*1.2)]\n",
    "fc1_size = sum(hidden_sizes) // 2\n",
    "dropout = 0.1\n",
    "output_size = 7\n",
    "weight_decay = 0.01\n",
    "\n",
    "model = LFLSTM(input_sizes, hidden_sizes, fc1_size, output_size, dropout)\n",
    "optimizer = Adam([param for param in model.parameters()\n",
    "                 if param.requires_grad], weight_decay=weight_decay)\n",
    "\n",
    "train_losses, valid_losses = train_model(model, train_loader, dev_loader, MAX_EPOCH, nn.CrossEntropyLoss(), optimizer, patience_n=25)\n",
    "model.load_state_dict(torch.load('model.std'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf4b96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test_model_classification(model, test_loader, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f7dd7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:36:41.396496Z",
     "start_time": "2022-05-23T13:36:41.228458Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0, len(train_losses)), train_losses, label=\"Train Loss\")\n",
    "plt.plot(np.arange(0, len(valid_losses)), valid_losses, label=\"Dev Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55dc0da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:38:43.067882Z",
     "start_time": "2022-05-23T13:38:15.997758Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "print(\"CUDA: \", CUDA)\n",
    "MAX_EPOCH = 1000\n",
    "\n",
    "text_size = 300\n",
    "visual_size = 35\n",
    "acoustic_size = 74\n",
    "pose_size = 25\n",
    "hand_r_size = 21\n",
    "hand_l_size = 21\n",
    "\n",
    "# define some model settings and hyper-parameters\n",
    "input_sizes = [text_size, visual_size, acoustic_size, pose_size, hand_r_size, hand_l_size]\n",
    "hidden_sizes = [int(text_size * 1.25), int(visual_size * 1.25),\n",
    "                int(acoustic_size * 1.25), int(pose_size*1.), int(hand_r_size*1.), int(hand_l_size*1.)]\n",
    "fc1_size = sum(hidden_sizes) // 2\n",
    "dropout = 0.05\n",
    "output_size = 4\n",
    "weight_decay = 0.1\n",
    "\n",
    "model = EFLSTM(input_sizes, hidden_sizes, output_size, dropout, n_layers=2, classification=True)\n",
    "optimizer = Adam([param for param in model.parameters()\n",
    "                 if param.requires_grad], weight_decay=weight_decay)\n",
    "\n",
    "train_losses, valid_losses = train_model(model, train_loader, dev_loader, MAX_EPOCH, nn.CrossEntropyLoss(), optimizer, patience_n=50)\n",
    "model.load_state_dict(torch.load('model.std'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718ae260",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:38:43.271928Z",
     "start_time": "2022-05-23T13:38:43.072884Z"
    }
   },
   "outputs": [],
   "source": [
    "results = test_model_classification(model, test_loader, nn.CrossEntropyLoss())\n",
    "plt.plot(np.arange(0, len(train_losses)), train_losses, label=\"Train Loss\")\n",
    "plt.plot(np.arange(0, len(valid_losses)), valid_losses, label=\"Dev Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training EFLSTM w/P\")\n",
    "plt.xlabel(\"N Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.savefig(\"train_graphs/plot.png\")\n",
    "# Test set MSE is 1.3169214725494385\n",
    "# [1.7539093e+00 4.1389942e-01 1.4151512e+00 1.1141731e+00 7.1338250e-04 4.9885786e-01 3.6192438e-01]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b992c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:39:00.831477Z",
     "start_time": "2022-05-23T13:38:43.276265Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "print(\"CUDA: \", CUDA)\n",
    "MAX_EPOCH = 1000\n",
    "\n",
    "text_size = 300\n",
    "visual_size = 35\n",
    "acoustic_size = 74\n",
    "pose_size = 25\n",
    "hand_r_size = 21\n",
    "hand_l_size = 21\n",
    "\n",
    "# define some model settings and hyper-parameters\n",
    "input_sizes = [text_size, visual_size, acoustic_size]#, pose_size, hand_r_size, hand_l_size]\n",
    "hidden_sizes = [int(text_size * 1.5), int(visual_size * 1.5),\n",
    "                int(acoustic_size * 1.5)]#, int(pose_size * 1.5), int(hand_r_size * 1.5), int(hand_l_size * 1.5)]\n",
    "fc1_size = sum(hidden_sizes) // 2\n",
    "dropout = 0.05\n",
    "output_size = 4\n",
    "weight_decay = 0.1\n",
    "\n",
    "model = EFLSTM(input_sizes, hidden_sizes, output_size, dropout, n_layers=2, classification=True)\n",
    "\n",
    "optimizer = Adam([param for param in model.parameters()\n",
    "                 if param.requires_grad], weight_decay=weight_decay)\n",
    "\n",
    "train_losses, valid_losses = train_model(model, train_loader, dev_loader, MAX_EPOCH, nn.CrossEntropyLoss(), optimizer, patience_n=50)\n",
    "model.load_state_dict(torch.load('model.std'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faa7af4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T13:39:01.053528Z",
     "start_time": "2022-05-23T13:39:00.835478Z"
    }
   },
   "outputs": [],
   "source": [
    "results = test_model_classification(model, test_loader, nn.CrossEntropyLoss())\n",
    "plt.plot(np.arange(0, len(train_losses)), train_losses, label=\"Train Loss\")\n",
    "plt.plot(np.arange(0, len(valid_losses)), valid_losses, label=\"Dev Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE OUT TESTING\n",
    "def lang_leave_one_out(dataset, dict_results=dict(), save_plots=True, label_n=0, output_size=1,  num_classes=-1, classification_task=False, graph_path=\".\"):\n",
    "    all_videos = list(videos)\n",
    "\n",
    "    CUDA = torch.cuda.is_available()\n",
    "    print(\"CUDA: \", CUDA)\n",
    "    MAX_EPOCH = 1000\n",
    "\n",
    "    text_size = 300\n",
    "\n",
    "    num_classes = num_classes\n",
    "    output_size = output_size\n",
    "    classification = classification_task\n",
    "    dropout_eflstm = 0.05\n",
    "    n_layer_eflstm = 2\n",
    "    weight_decay = 0.1\n",
    "\n",
    "    dict_results[\"videos\"] = all_videos\n",
    "    for test_i in range(0, len(all_videos)):\n",
    "        test_v = [all_videos[test_i]]\n",
    "        for dev_i in range(0, len(all_videos)):\n",
    "            if dev_i != test_i:\n",
    "                print(\"Dev i:\", dev_i, \" || Test i: \", test_i,)\n",
    "                dev_v = [all_videos[dev_i]]\n",
    "                train_v = [v for v in all_videos if v not in test_v and v not in dev_v]\n",
    "                train_s, dev_s, test_s = get_train_dev_test_splits(dataset, train_v, test_v, dev_v, label_n=label_n, n_classes=num_classes)\n",
    "                if len(dev_s) == 0 or len(test_s) == 0:\n",
    "                    print(\"WARNING 0 SEGMENTS in TEST OR DEV, skipping...\")\n",
    "                    continue\n",
    "                if classification_task:\n",
    "                    train_loader = DataLoader(train_s, shuffle=False, batch_size=32, collate_fn=multi_collate_class)\n",
    "                    dev_loader = DataLoader(dev_s, shuffle=False, batch_size=32, collate_fn=multi_collate_class)\n",
    "                    test_loader = DataLoader(test_s, shuffle=False, batch_size=32, collate_fn=multi_collate_class)\n",
    "                else:\n",
    "                    train_loader = DataLoader(train_s, shuffle=False, batch_size=32, collate_fn=multi_collate_reg)\n",
    "                    dev_loader = DataLoader(dev_s, shuffle=False, batch_size=32, collate_fn=multi_collate_reg)\n",
    "                    test_loader = DataLoader(test_s, shuffle=False, batch_size=32, collate_fn=multi_collate_reg)\n",
    "                print(np.array(train_s).shape, np.array(dev_s).shape, np.array(test_s).shape)\n",
    "\n",
    "                eflstm_settings = ( [text_size,], \n",
    "                                    [int(text_size * 1.5),],\n",
    "                                    output_size,\n",
    "                                    dropout_eflstm,\n",
    "                                    MAX_EPOCH,\n",
    "                                    n_layer_eflstm,\n",
    "                                    weight_decay,\n",
    "                                    classification\n",
    "                                )\n",
    "\n",
    "                dict_results[(\"LangLSTM\",test_i,dev_i)] = LangLSTM_run(train_loader, dev_loader, test_loader, *eflstm_settings)\n",
    "                train_losses, valid_losses, _ = dict_results[(\"LangLSTM\",test_i,dev_i)]\n",
    "                if save_plots:\n",
    "                    plt.plot(np.arange(0, len(train_losses)), train_losses, label=\"Train Loss\")\n",
    "                    plt.plot(np.arange(0, len(valid_losses)), valid_losses, label=\"Dev Loss\")\n",
    "                    plt.legend()\n",
    "                    plt.title(f\"Training LangLSTM, Dev: {dev_i} | Test: {test_i}\")\n",
    "                    plt.xlabel(\"N Epochs\")\n",
    "                    plt.ylabel(\"Loss\")\n",
    "                    plt.savefig(os.path.join(graph_path, \"LangLSTM\", f\"LangLSTM_t{test_i}_d{dev_i}.png\"))\n",
    "                    plt.clf()\n",
    "                \n",
    "    return dict_results                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e5b359",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_dictionary = dict()\n",
    "results_dictionary_top10_langs2c = lang_leave_one_out(dataset, saved_dictionary, save_plots=False, label_n=1, output_size=4, \n",
    "                                    num_classes=4, classification_task=True, graph_path=os.path.join(\"train_graphs\",\"Happy_4c\"),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ff871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26de32e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_percent(n, decimals=2):\n",
    "    return np.round(n*100, decimals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396007a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_f1_acc(results):\n",
    "    single_weighted_acc = [balanced_accuracy_score(results[key][2][1], results[key][2][0]) for key in results.keys() if key != \"videos\"]\n",
    "    single_weighted_f1 = [results[key][2][2][0] for key in results.keys() if key != \"videos\"]\n",
    "    print(\"Weighted Acc:\", to_percent(np.array(single_weighted_acc).mean()))\n",
    "    print(\"Average F1:\", to_percent(np.array(single_weighted_f1).mean()))\n",
    "\n",
    "    return single_weighted_acc, single_weighted_f1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1219353",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_f1_acc(results_dictionary_top10_langs2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251430b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
